# -*- coding: utf-8 -*-
"""Dry_Beans_Classifications.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l7TEnA_pzkccT1U__FAEQi31IJBresWy



# **Introdution**
There are seven main types of dry beans. Included barbunya, sira, horoz, dermason, cali, bombay, and seker. Each type can be distinguished by many features such as the area of the bean zone, bean circumference, the distance between the ends of the longest line that can be drawn from a bean, and many more.
In this project we will classify a type of beans based on their features. Three classifiers will be applied, then we compare the performance of them. 
The dataset that will be used in this project is Dry Beans Dataset, from Kaggle.
https://www.kaggle.com/datasets/whenamancodes/dry-beans-dataset?select=Dry+Beans+Dataset.csv

1. Import required libraries
"""

# math lib
import numpy as np 

# data visualization libs
import pandas as pd
from matplotlib import pyplot as plt
from matplotlib.colors import ListedColormap
import seaborn as sns 
from mlxtend.plotting import scatterplotmatrix

# feature scaling libs
from sklearn.preprocessing import MinMaxScaler, StandardScaler

# feature selection libs
from sklearn.feature_selection import SequentialFeatureSelector

# model selection lib
from sklearn.model_selection import train_test_split, GridSearchCV

# classification models
from sklearn.neighbors import KNeighborsClassifier as KNN , NearestCentroid as KNC
from sklearn.tree import DecisionTreeClassifier as DTC
from sklearn.svm import SVC

# metrics for model performance evaluation
from sklearn.metrics import accuracy_score, classification_report, f1_score

"""# **Data Description**
The dataset consists of features describing the shape of the beans. Where, the number of features in the data is 16, and the number of class is 7. Below is the attribute describtion.

1.) Area (A): The area of a bean zone and the number of pixels within its boundaries.

2.) Perimeter (P): Bean circumference is defined as the length of its border.

3.) Major axis length (L): The distance between the ends of the longest line that can be drawn from a bean.

4.) Minor axis length (l): The longest line that can be drawn from the bean while standing perpendicular to the main axis.

5.) Aspect ratio (K): Defines the relationship between L and l.

6.) Eccentricity (Ec): Eccentricity of the ellipse having the same moments as the region.

7.) Convex area (C): Number of pixels in the smallest convex polygon that can contain the area of a bean seed.

8.) Equivalent diameter (Ed): The diameter of a circle having the same area as a bean seed area.

9.) Extent (Ex): The ratio of the pixels in the bounding box to the bean area.

10.)Solidity (S): Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.

11.)Roundness (R): Calculated with the following formula: (4piA)/(P^2).

12.)Compactness (CO): Measures the roundness of an object: Ed/L.

13.)ShapeFactor1 (SF1).

14.)ShapeFactor2 (SF2).

15.)ShapeFactor3 (SF3).

16.)ShapeFactor4 (SF4).

17.)Class (Seker, Barbunya, Bombay, Cali, Dermosan, Horoz and Sira)


"""

df = pd.read_csv('/content/sample_data/Dry_Beans_Dataset.csv') #read data
df.head() #Disply some records

df.shape #get the shape of data

df.dtypes #get the type of columns

df.describe() #describe the data

df.info() #get more information about the data

"""From above information, we can see that the data has no null values.

The plot above shows the imbalanced in data. Where, the count of dermason class is highst than the other classes. Wherese, the count of bombay class is lowest than the other classes. Due to this, we are going to use **F1-score** metrics to evaluate the performance of the classifiers, in addition to accuracy.

# **Data Visualization**
The graphical display of information and data is known as data visualization. Data visualization tools, which include visual components such as charts, graphs, and maps, make it easy to view and comprehend trends, outliers, and patterns in data. In our project, we will use seaborn library from sklearn to generate some charts about the data. First, we generate the count plot to see the number of each class, and help us to see if there is any imbalanced in data. Then, we generate the scatter plot matrix to see the relation between the features. Also, we provides a heatmap to generate the pearson correlation between the features.
"""

#first, we define the X (features), and y (Classes).
#make a copy of the dataset
df2=df.copy()
# convert column 'Class' from object to category
df2['Class'] = df2['Class'].astype('category')
# encode the categories of seven classes to numerical values
df2['Class'] = df2['Class'].cat.codes
print(df2.head())
#set x and y
X = df2.iloc[:,:-1].values
y = df2.iloc[:,-1].values
print(y.shape)

# Check the Count of all beans classes
print(df['Class'].value_counts())
sns.countplot(x='Class', data=df)

#show the scatter plot of whole features
scatterplotmatrix(X, figsize=(50, 50))
plt.tight_layout()
plt.show()

#shows the pearson correlation between the features
plt.figure(figsize=(12,12))
sns.heatmap(df2.corr("pearson"),vmin=-1, vmax=1,cmap='coolwarm',annot=True, square=True)

"""# **Data pre-processing**
In the next steps, we are going to prepare the data to do the classification. First, we define a function to do the data transformation. Then, we use this function to do scaling to data. Finally, we split the data into train set and test set.
"""

# Check if there are missing values

if df2.isnull().values.any():
    df2 = df2.dropna() #deleting the corresponding rows 

X = df2.iloc[:,:-1].values
y = df2.iloc[:,-1].values

# Scaling function
def feature_scaling(x, scaler_name= 'minmax'):
  # create the scaler object
  if scaler_name.lower() == 'minmax':
    scaler = MinMaxScaler()
  if scaler_name.lower() == 'zscore':
    scaler = StandardScaler()
  # fit it to data features
  x_scaled = scaler.fit_transform(x)
  return x_scaled

# perform feature scaling using minMax scalers
X_scaled = feature_scaling(X)

### Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled,y,test_size=0.3)
print("Train size: ",X_train.shape,'\n','Test size: ', X_test.shape)

"""# **Model Selection**
In order to select the best model, we select the best features sequentially, also, we select the best hyperparameter for each model. To do this, we defines two functions. The first function, return the best features. The second function, reuturn the best parameter.
"""

# function to select the best features and return the reduced data
# the fuction uses the the model and the number of feature to be selected
def feature_selection(model, n_features):
  sfs = SequentialFeatureSelector(model, n_features_to_select= n_features, direction = 'forward')
  sfs.fit(X_train, y_train)
  X_train_reduced = sfs.transform(X_train)
  X_test_reduced = sfs.transform(X_test)
  return X_train_reduced, X_test_reduced

#define a function to select the best hyperparameter of the model
#the function uses the model and parameter values, with k-fold=10
def best_param(model, parameter):
  grid_search=GridSearchCV(model,parameter,scoring='accuracy',cv=10, n_jobs= -1)
  grid_search.fit(X_train,y_train)
  l=[*parameter]
  #return the best parameter
  return grid_search.best_params_[l[0]]

"""# **KNN Classifier**"""

## KNN before features selection.
#intialize knn object
knn = KNN(n_neighbors=1)
#fit the model
knn.fit(X_train, y_train)
#evaluate the model
y_pred = knn.predict(X_test)
#compute f-score with micro average
f1_score_knn = f1_score(y_test,y_pred, average= 'micro')
#compute the accuaracy
accuracy_knn = accuracy_score(y_pred, y_test)
print(f"Knn f1_score before features selection = {f1_score_knn:.2f}")
print(f"Knn accuracy before features selection = {accuracy_knn:.2f}")

"""From above results, we can see that the model accuracy is identical to f-score value. From accuracy, the model is able to predict 90% of beans type correctly. The error rate of the model is 10%."""

### dimensionality reduction using feature selection
knn1 = KNN(n_neighbors=1)
#select the best 10 features
X_train_reduced, X_test_reduced = feature_selection(knn1, 10)
knn1.fit(X_train_reduced, y_train)#fit the model
y_pred_sfs = knn1.predict(X_test_reduced)#evaluate the model
#compute f-score and the accuracy of the model
f1_score_knn1 = f1_score(y_test,y_pred_sfs, average= 'micro')
accuracy_knn1 = accuracy_score(y_pred_sfs, y_test)
print(f"Knn f1_score after applying feature selection = {f1_score_knn1:.2f}")
print(f"Knn accuracy after applying feature selection = {accuracy_knn1:.2f}")

"""Above results shows that the f-score and the accuracy of knn is identical to that when there is no features selection."""

### Grid Serach to find the best k
knn2 = KNN()
k_range = {'n_neighbors':list(range(1,20,1))}
best_k = best_param(knn2, k_range)
print("The best k for knn is: ",best_k)
#fit the model using the best k
knn2 = KNN(n_neighbors=best_k)
#fit the model
knn2.fit(X_train_reduced, y_train)
#evaluation
y_pred2 =knn2.predict(X_test_reduced)
#compute the accuracy and f-score
f1_score_knn2 = f1_score(y_test,y_pred2, average= 'micro')
accuracy_knn2 = accuracy_score(y_pred2, y_test)
print(f"Knn f1_score after using the best parameter = {f1_score_knn2:.2f}")
print(f"Knn accuracy after using the best parameter = {accuracy_knn2:.2f}")

"""From above result, we can see that the performance of knn is increasing, when we use the best k nighbors.

# **NearestCentroid Classifier**
"""

## NearestCentroid before using feature selection and grid search
knc = KNC()
knc.fit(X_train, y_train)
y_pred = knc.predict(X_test)
f1_score_knc = f1_score(y_test,y_pred, average= 'micro')
accuracy_knc = accuracy_score(y_pred, y_test)

print(f"f1_score before feature selection of scaled dataset = {f1_score_knc:.2f}")
print(f"Accuracy before feature selection of scaled dataset = {accuracy_knc:.2f}")

### dimensionality reduction using feature selection
knc1 = KNC()
#select the best 10 features
X_train_reduced, X_test_reduced = feature_selection(knc1, 10)
knc1.fit(X_train_reduced, y_train)#fit the model
y_pred_sfs = knc1.predict(X_test_reduced) #evaluate the model
#compute f1-score and the accuracy of the model
f1_score_knc1 = f1_score(y_test,y_pred_sfs, average= 'micro')
accuracy_knc1 = accuracy_score(y_pred_sfs, y_test)
print(f"Knc f1_score after applying feature selection = {f1_score_knc1:.2f}")
print(f"Knc accuracy after applying feature selection = {accuracy_knc1:.2f}")

"""The result above show that the performane of KNC is increasing, when we use the best 10 features. Where the accuracy and f1-score reaced 90%."""

### Grid Serach to find the best threshold
knc2 = KNC()
grid = dict()
grid['shrink_threshold'] = np.arange(0, 1.01, 0.01)
best_shold = best_param( knc2 , grid)
print("The best threshold for knc is: ",best_shold)
#fit the model using the best THRESHOLD
knc2 = KNC(shrink_threshold=best_shold)
#fit the model
knc2.fit(X_train_reduced, y_train)
#evaluation
y_pred2 =knc2.predict(X_test_reduced)
#compute the accuracy and f1-score
f1_score_knc2 = f1_score(y_test,y_pred2, average= 'micro')
accuracy_knc2 = accuracy_score(y_pred2, y_test)
print(f"Knc f1_score after using the best threshold = {f1_score_knc2:.2f}")
print(f"Knn accuracy after using the best threshold = {accuracy_knc2:.2f}")

"""The result above show that the performance of knc is increasing when we use the best shrink threshold, compared to one that is not using shrink threshold.

# **Decision Tree Classifier**
"""

## Decision Tree Classification
dtc = DTC(random_state=0)
dtc.fit(X_train, y_train)
y_pred = dtc.predict(X_test)
f1_score_dtc =f1_score(y_test,y_pred, average= 'micro')
accuracy_dtc = accuracy_score(y_pred, y_test)
print(f" DTC f1-score before feature selection of scaled dataset = {f1_score_dtc:.2f}")
print(f" DTC accuracy before feature selection of scaled dataset = {accuracy_dtc:.2f}")

### dimensionality reduction using feature selection
dtc1 = DTC()
#select the best 10 features
X_train_reduced, X_test_reduced = feature_selection(dtc1, 10)
dtc1.fit(X_train_reduced, y_train)#fit the model
y_pred_sfs = dtc1.predict(X_test_reduced)#evaluate the model
#compute f-score and the accuracy of the model
f1_score_dtc1 = f1_score(y_test,y_pred_sfs, average= 'micro')
accuracy_dtc1 = accuracy_score(y_pred_sfs, y_test)
print(f"DTC accuracy after applying feature selection = {accuracy_dtc1:.2f}")
print(f"DTC f1_score after applying feature selection = {f1_score_dtc1:.2f}")

"""From above results, we can see that the performance of decision tree classifier is keep same after select the best 10 features."""

# Grid search to select the best maximum depth for decision tree classifier
dtc2 = DTC()
dtc_params={'max_depth':list(range(4,20))}
best_depth = best_param( dtc2 , dtc_params)
print("The best maximum depth for dtc is: ",best_depth)
#fit the model using the best alpha
dtc2 = DTC(max_depth=best_depth)
#fit the model
dtc2.fit(X_train, y_train)
#evaluation
y_pred2 =dtc2.predict(X_test)
#compute the accuracy and f-score
f1_score_dtc2 = f1_score(y_test,y_pred2, average= 'micro')
accuracy_dtc2 = accuracy_score(y_pred2, y_test)
print(f"Dtc f1_score after using the best maximum depth = {f1_score_dtc2:.2f}")
print(f"Dtc accuracy after using the best maximum depth= {accuracy_dtc2:.2f}")

"""From the result above we can see that the performance of decision tree classifier is increasing after selecting the best maximum depth.

# **Support Vector Classifier**
"""

## LogisticRegression before using feature selection and grid search
svc = SVC()
svc.fit(X_train, y_train)
accuracy_svc = svc.score(X_test, y_test)
print(f"Score before feature selection of scaled dataset = {accuracy_svc:.2f}")

### dimensionality reduction using feature selection
svc1 = SVC()
#select the best 10 features
X_train_reduced, X_test_reduced = feature_selection(svc1, 10)
svc1.fit(X_train_reduced, y_train)#fit the model
#compute score and the accuracy of the model
accuracy_svc1 = svc1.score(X_test_reduced, y_test)
print(f"SVC accuracy after applying feature selection = {accuracy_svc1:.2f}")

# Grid search to select the best maximum depth for decision tree classifier
svc2 = SVC()
svc_params={'C': list(range(1,100,10))}

best_C = best_param( svc2 , svc_params)
print("The best C for SVC is: ",best_C)
#fit the model using the best alpha
svc2 = SVC(C=best_C)
#fit the model
svc2.fit(X_train, y_train)
#evaluation
#compute the accuracy and f-score
accuracy_svc2 = svc2.score(X_test,y_test)
print(f"SVC score after using the best C = {accuracy_svc2:.2f}")

# display the result as DataFrame
dict = {'K Nearest Neighbors': [accuracy_knn, accuracy_knn1, accuracy_knn2 ],
        'K Nearest Centroid' : [accuracy_knc, accuracy_knc1, accuracy_knc2 ],
        'Decision Tree' : [accuracy_dtc, accuracy_dtc1, accuracy_dtc2 ],
        'Support Vector ' :[accuracy_svc, accuracy_svc, accuracy_svc2 ]
        }
df = pd.DataFrame(dict)
print(df)